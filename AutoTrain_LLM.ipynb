{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "JvMRbVLEJlZT",
        "outputId": "7ca46d2b-17fd-463c-d0cb-a292e3773d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoTrain version: 0.8.24\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ¤— AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "!autotrain setup --colab > setup_logs.txt\n",
        "from autotrain import __version__\n",
        "print(f'AutoTrain version: {__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'my-autotrain-llm' # @param {type:\"string\"}\n",
        "model_name = 'abhishek/llama-2-7b-hf-small-shards' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"hf_XXX\" #@param {type:\"string\"}\n",
        "hf_username = \"abc\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "unsloth = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 1024 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"generic\", \"sft\"] {type:\"string\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"string\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"string\"}\n",
        "lora_r = 16 #@param {type:\"number\"}\n",
        "lora_alpha = 32 #@param {type:\"number\"}\n",
        "lora_dropout = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"HF_USERNAME\"] = hf_username\n",
        "\n",
        "conf = f\"\"\"\n",
        "task: llm-{trainer}\n",
        "base_model: {model_name}\n",
        "project_name: {project_name}\n",
        "log: tensorboard\n",
        "backend: local\n",
        "\n",
        "data:\n",
        "  path: data/\n",
        "  train_split: train\n",
        "  valid_split: null\n",
        "  chat_template: null\n",
        "  column_mapping:\n",
        "    text_column: text\n",
        "\n",
        "params:\n",
        "  block_size: {block_size}\n",
        "  lr: {learning_rate}\n",
        "  warmup_ratio: {warmup_ratio}\n",
        "  weight_decay: {weight_decay}\n",
        "  epochs: {num_epochs}\n",
        "  batch_size: {batch_size}\n",
        "  gradient_accumulation: {gradient_accumulation}\n",
        "  mixed_precision: {mixed_precision}\n",
        "  peft: {peft}\n",
        "  quantization: {quantization}\n",
        "  lora_r: {lora_r}\n",
        "  lora_alpha: {lora_alpha}\n",
        "  lora_dropout: {lora_dropout}\n",
        "  unsloth: {unsloth}\n",
        "\n",
        "hub:\n",
        "  username: ${{HF_USERNAME}}\n",
        "  token: ${{HF_TOKEN}}\n",
        "  push_to_hub: {push_to_hub}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"conf.yaml\", \"w\") as f:\n",
        "  f.write(conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "g3cd_ED_yXXt",
        "outputId": "26ad5334-d9bb-4194-e721-a5fb7d1a9105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.cli.autotrain\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mUsing AutoTrain configuration: conf.yaml\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mRunning task: lm_training\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mUsing backend: local\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m{'model': 'abhishek/llama-2-7b-hf-small-shards', 'project_name': 'my-autotrain-llm', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': None, 'text_column': 'text', 'rejected_text_column': None, 'push_to_hub': False, 'username': 'abc', 'token': '*****', 'unsloth': False, 'distributed_backend': None}\u001b[0m\n",
            "\rSaving the dataset (0/1 shards):   0% 0/1 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 86.29 examples/s]\rSaving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 85.27 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/1 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 435.14 examples/s]\rSaving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 411.53 examples/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m523\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'my-autotrain-llm/training_params.json']\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:03\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m524\u001b[0m - \u001b[1m{'model': 'abhishek/llama-2-7b-hf-small-shards', 'project_name': 'my-autotrain-llm', 'data_path': 'my-autotrain-llm/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': False, 'username': 'abc', 'token': '*****', 'unsloth': False, 'distributed_backend': None}\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 5, in <module>\n",
            "    from accelerate.commands.accelerate_cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
            "    from accelerate.commands.estimate import estimate_command_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/estimate.py\", line 34, in <module>\n",
            "    import timm\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/__init__.py\", line 2, in <module>\n",
            "    from .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/__init__.py\", line 8, in <module>\n",
            "    from .classifier import ClassifierHead, create_classifier, NormMlpClassifierHead\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/classifier.py\", line 15, in <module>\n",
            "    from .create_norm import get_norm_layer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/create_norm.py\", line 14, in <module>\n",
            "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
            "    @torch.library.register_fake(\"torchvision::nms\")\n",
            "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-16 14:16:06\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mJob ID: 2733\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain --config conf.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}